{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c4l38LgJ3hH"
      },
      "source": [
        "## **Prerequsit**\n",
        "---\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvPZ3VkLIVFv",
        "outputId": "a325caf1-171d-4ab5-c746-8d99c7a89ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow>=2.8.0\n",
        "!pip install numpy pandas matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vMqPK6qKOGK"
      },
      "source": [
        "### Required Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "VidvVzwzJ9R2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqnGKADKo8K"
      },
      "source": [
        "## **Data preprocessing**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wtp8v9MLnpP",
        "outputId": "f7fd2783-1bb4-4a91-8c2f-7c326d8058fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_and_extract_dataset():\n",
        "    \"\"\"Download English-French dataset from Anki with a User-Agent header to avoid 406 error.\"\"\"\n",
        "    url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "    zip_path = \"fra-eng.zip\"\n",
        "\n",
        "    # Define a standard User-Agent to prevent the 406 'Not Acceptable' error\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # Create the Request object\n",
        "    req = urllib.request.Request(url, headers=headers)\n",
        "\n",
        "    # Download dataset\n",
        "    try:\n",
        "        # Open the URL with the custom request\n",
        "        with urllib.request.urlopen(req) as response, open(zip_path, 'wb') as out_file:\n",
        "            # Write the content to the local zip file\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print(f\"HTTP Error encountered: {e.code} - {e.reason}\")\n",
        "        # Clean up the partial file if it exists\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "        return # Stop execution if download fails\n",
        "\n",
        "    # Extract dataset\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "\n",
        "    # Clean up\n",
        "    os.remove(zip_path)\n",
        "    print(\"Dataset downloaded and extracted successfully!\")\n",
        "\n",
        "# Uncomment to download\n",
        "download_and_extract_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhNK0D1lMte1"
      },
      "source": [
        "## **Text Preprocessing**\n",
        "---\n",
        "\n",
        "### **Text Cleaning and Normalization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AZK4PepKSm8",
        "outputId": "5f6ae242-8966-495e-e1d7-9509793f8eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "before Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\n",
            "before Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\n",
            "before Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\n",
            "before Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "before Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "before Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "before Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "before Run!\tPrenez vos jambes à vos cous !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)\n",
            "before Run!\tFile !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)\n",
            "after ['<start> go . <end>', '<start> va ! <end>']\n",
            "after ['<start> go . <end>', '<start> marche . <end>']\n",
            "after ['<start> go . <end>', '<start> en route ! <end>']\n",
            "after ['<start> go . <end>', '<start> bouge ! <end>']\n",
            "after ['<start> hi . <end>', '<start> salut ! <end>']\n",
            "after ['<start> hi . <end>', '<start> salut . <end>']\n",
            "after ['<start> run ! <end>', '<start> cours ! <end>']\n",
            "after ['<start> run ! <end>', '<start> courez ! <end>']\n",
            "after ['<start> run ! <end>', '<start> prenez vos jambes a vos cous ! <end>']\n",
            "after ['<start> run ! <end>', '<start> file ! <end>']\n",
            "<zip object at 0x77fe014a9300>\n"
          ]
        }
      ],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    \"\"\"Convert unicode string to ascii\"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    \"\"\"Clean and preprocess sentence\"\"\"\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # Add spaces around punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # Replace non-letter characters with space (except punctuation)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # Add start and end tokens\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "def create_dataset(path, num_examples=50000):\n",
        "    \"\"\"Create preprocessed dataset\"\"\"\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    for i in range(10):\n",
        "      print(\"before\",lines[i])\n",
        "\n",
        "    word_pairs = []\n",
        "    for line in lines[:num_examples]:\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            english = preprocess_sentence(parts[0])\n",
        "            french = preprocess_sentence(parts[1])\n",
        "            word_pairs.append([english, french])\n",
        "\n",
        "\n",
        "    for j in range(10):\n",
        "      print(\"after\", word_pairs[j])\n",
        "\n",
        "\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "\n",
        "test = create_dataset('/content/fra.txt', num_examples=30000)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO_-nOpXOD_m"
      },
      "source": [
        "### **Tokenization and Vocabulary Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oRYMzJFnODtK"
      },
      "outputs": [],
      "source": [
        "class LanguageTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "\n",
        "    def create_tokenizer(self, dataset):\n",
        "        \"\"\"Create tokenizer from dataset\"\"\"\n",
        "        for sentence in dataset:\n",
        "            for word in sentence.split(' '):\n",
        "                if word not in self.vocab:\n",
        "                    self.vocab.add(word)\n",
        "\n",
        "        # Sort vocabulary\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # Create word to index mapping\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "\n",
        "        # Create index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word\n",
        "\n",
        "        return self.word2idx, self.idx2word\n",
        "\n",
        "    def encode(self, sentence, max_length):\n",
        "        \"\"\"Convert sentence to sequence of indices\"\"\"\n",
        "        sequence = [self.word2idx.get(word, 0) for word in sentence.split(' ')]\n",
        "        sequence = sequence[:max_length]\n",
        "        sequence += [0] * (max_length - len(sequence))\n",
        "        return sequence\n",
        "\n",
        "    def decode(self, sequence):\n",
        "        \"\"\"Convert sequence of indices back to sentence\"\"\"\n",
        "        return ' '.join([self.idx2word.get(idx, '<unk>') for idx in sequence if idx != 0])\n",
        "\n",
        "def create_tokenizers_and_datasets(en_sentences, fr_sentences):\n",
        "    \"\"\"Create tokenizers and convert sentences to sequences\"\"\"\n",
        "    # Create tokenizers\n",
        "    en_tokenizer = LanguageTokenizer()\n",
        "    fr_tokenizer = LanguageTokenizer()\n",
        "\n",
        "    en_tokenizer.create_tokenizer(en_sentences)\n",
        "    fr_tokenizer.create_tokenizer(fr_sentences)\n",
        "\n",
        "    # Find max length\n",
        "    en_max_length = max(len(sentence.split(' ')) for sentence in en_sentences)\n",
        "    fr_max_length = max(len(sentence.split(' ')) for sentence in fr_sentences)\n",
        "\n",
        "    print(f'English max length: {en_max_length}')\n",
        "    print(f'French max length: {fr_max_length}')\n",
        "    print(f'English vocab size: {len(en_tokenizer.word2idx)}')\n",
        "    print(f'French vocab size: {len(fr_tokenizer.word2idx)}')\n",
        "\n",
        "    # Convert to sequences\n",
        "    input_tensor = np.array([en_tokenizer.encode(sentence, en_max_length)\n",
        "                           for sentence in en_sentences])\n",
        "    target_tensor = np.array([fr_tokenizer.encode(sentence, fr_max_length)\n",
        "                            for sentence in fr_sentences])\n",
        "\n",
        "    return input_tensor, target_tensor, en_tokenizer, fr_tokenizer, en_max_length, fr_max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMeu6DWHPD0D"
      },
      "source": [
        "## **Model Architecture**\n",
        "---\n",
        "### **Encoder Class**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "7ytk4XdEPMM5"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(enc_units,\n",
        "                               return_sequences=True,\n",
        "                               return_state=True,\n",
        "                               recurrent_initializer='glorot_uniform'),\n",
        "            merge_mode='sum'\n",
        "        )\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # x shape: (batch_size, max_length)\n",
        "        x = self.embedding(x)\n",
        "        # x shape after embedding: (batch_size, max_length, embedding_dim)\n",
        "\n",
        "        output, forward_h, forward_c, backward_h, backward_c = self.lstm(x, initial_state=hidden)\n",
        "\n",
        "        # Combine forward and backward states\n",
        "        state_h = tf.keras.layers.Add()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Add()([forward_c, backward_c])\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # For bidirectional LSTM, we need states for both directions\n",
        "        return [tf.zeros((self.batch_size, self.enc_units)) for _ in range(4)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeineBCMPbme"
      },
      "source": [
        "### **Attention Mechanism (Luong Attention)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "tCx8cc0YPSJX"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.units = units\n",
        "        # Attention weight matrix\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query shape: (batch_size, hidden_size)\n",
        "        # values shape: (batch_size, max_len, hidden_size)\n",
        "\n",
        "        # Expand query to match values dimensions for broadcasting\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        # query_with_time_axis shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Calculate attention scores using matrix multiplication\n",
        "        # score shape: (batch_size, max_length, 1)\n",
        "        score = tf.matmul(values, query_with_time_axis, transpose_b=True)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Calculate context vector\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VeTYv06Pjy7"
      },
      "source": [
        "### **Decoder Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LvYs2bHfPgU-"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, attention_type='luong'):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        self.attention = LuongAttention(dec_units)\n",
        "\n",
        "        # Final output layer\n",
        "        self.Wc = tf.keras.layers.Dense(dec_units, activation='tanh')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # x shape: (batch_size, 1)\n",
        "        # hidden shape: (batch_size, dec_units)\n",
        "        # enc_output shape: (batch_size, max_length, hidden_size)\n",
        "\n",
        "        # Pass through embedding\n",
        "        x = self.embedding(x)\n",
        "        # x shape after embedding: (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
        "        # output shape: (batch_size, 1, dec_units)\n",
        "\n",
        "        # Remove time dimension for attention\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # Calculate attention\n",
        "        context_vector, attention_weights = self.attention(output, enc_output)\n",
        "\n",
        "        # Combine context vector and decoder output\n",
        "        output = tf.concat([tf.expand_dims(context_vector, 1),\n",
        "                           tf.expand_dims(output, 1)], axis=-1)\n",
        "\n",
        "        # Apply final transformations\n",
        "        output = self.Wc(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # Final prediction\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, [state_h, state_c], attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pkGInb4PqCr"
      },
      "source": [
        "### **Complete Model Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "p2pVZ_IcPtIC"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderModel(tf.keras.Model):\n",
        "    def __init__(self, enc_vocab_size, dec_vocab_size, embedding_dim,\n",
        "                 enc_units, dec_units, batch_size):\n",
        "        super(EncoderDecoderModel, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(enc_vocab_size, embedding_dim, enc_units, batch_size)\n",
        "        self.decoder = Decoder(dec_vocab_size, embedding_dim, dec_units, batch_size)\n",
        "\n",
        "        self.dec_vocab_size = dec_vocab_size\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        enc_input, dec_input = inputs\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        enc_hidden = self.encoder.initialize_hidden_state()\n",
        "\n",
        "        # Encode\n",
        "        enc_output, enc_h, enc_c = self.encoder(enc_input, enc_hidden)\n",
        "\n",
        "        # Initialize decoder hidden state with encoder states\n",
        "        dec_hidden = [enc_h, enc_c]\n",
        "\n",
        "        # Decode\n",
        "        predictions = []\n",
        "        attention_weights_list = []\n",
        "\n",
        "        for t in range(dec_input.shape[1]):\n",
        "            dec_input_t = tf.expand_dims(dec_input[:, t], 1)\n",
        "\n",
        "            predictions_t, dec_hidden, attention_weights = self.decoder(\n",
        "                dec_input_t, dec_hidden, enc_output)\n",
        "\n",
        "            predictions.append(predictions_t)\n",
        "            attention_weights_list.append(attention_weights)\n",
        "\n",
        "        # Stack predictions\n",
        "        predictions = tf.stack(predictions, axis=1)\n",
        "\n",
        "        return predictions, attention_weights_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkAOZCpHPyK5"
      },
      "source": [
        "## **Training**\n",
        "---\n",
        "\n",
        "### **Loss Function and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Rt4KlItEPuiX"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    \"\"\"Custom loss function that ignores padding tokens\"\"\"\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "    loss = loss_object(real, pred)\n",
        "\n",
        "    # Create mask to ignore padding tokens\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K83tbTJnQCpK"
      },
      "source": [
        "### **Training Steps**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "6URfQpeCP-im"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, model):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        dec_input = targ[:, :-1]\n",
        "        dec_target = targ[:, 1:]\n",
        "\n",
        "        predictions, attention_weights = model([inp, dec_input], training=True)\n",
        "\n",
        "        loss = loss_function(dec_target, predictions)\n",
        "\n",
        "    variables = model.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # Gradient clipping\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC9DxqSjQL-R"
      },
      "source": [
        "### **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "2e6ykY1aQJI3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset, epochs, steps_per_epoch):\n",
        "    \"\"\"Train the model\"\"\"\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Create checkpoints directory if it doesn't exist\n",
        "    if not os.path.exists('checkpoints'):\n",
        "        os.makedirs('checkpoints')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(inp, targ, model)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "        print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "        # Save checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model.save_weights(f'checkpoints/ckpt-{epoch+1}.weights.h5')\n",
        "            print(f'Checkpoint saved at epoch {epoch+1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qJQJKWbSoQB"
      },
      "source": [
        "## **Inference and Translation**\n",
        "---\n",
        "\n",
        "\n",
        "### **Translation Function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZmI8qcZSvKu"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr):\n",
        "    \"\"\"Translate a sentence from English to French\"\"\"\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [en_tokenizer.encode(sentence, max_length_en)]\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    hidden = model.encoder.initialize_hidden_state()\n",
        "    enc_out, enc_h, enc_c = model.encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = [enc_h, enc_c]\n",
        "    dec_input = tf.expand_dims([fr_tokenizer.word2idx['<start>']], 0)\n",
        "\n",
        "    # Initialize attention_plot with correct dimensions\n",
        "    attention_plot = np.zeros((max_length_fr, inputs.shape[1]))\n",
        "\n",
        "    for t in range(max_length_fr):\n",
        "        predictions, dec_hidden, attention_weights = model.decoder(\n",
        "            dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # Store attention weights for plotting\n",
        "        # Reshape attention_weights to (max_length_en,) for storing\n",
        "        attention_plot[t] = tf.squeeze(attention_weights).numpy()\n",
        "\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += fr_tokenizer.idx2word.get(predicted_id, '<unk>') + ' '\n",
        "\n",
        "        if fr_tokenizer.idx2word.get(predicted_id, '<unk>') == '<end>':\n",
        "            break\n",
        "\n",
        "        # The predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result.strip(), sentence, attention_plot\n",
        "\n",
        "def evaluate_translation(sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr):\n",
        "    \"\"\"Evaluate a single translation\"\"\"\n",
        "    result, sentence, attention_plot = translate(sentence, model, en_tokenizer,\n",
        "                                               fr_tokenizer, max_length_en, max_length_fr)\n",
        "\n",
        "    print(f'Input: {sentence}')\n",
        "    print(f'Predicted translation: {result}')\n",
        "\n",
        "    return result, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7B-KzsVSzop"
      },
      "source": [
        "## **Evaluation**\n",
        "---\n",
        "\n",
        "### **BLEU Score Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW2EgsZ0SxAV"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def calculate_bleu_score(reference, candidate, n=4):\n",
        "    \"\"\"Calculate BLEU score for a single sentence pair\"\"\"\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    if len(candidate_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate precision for n-grams\n",
        "    precisions = []\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        ref_ngrams = Counter([' '.join(reference_tokens[j:j+i])\n",
        "                             for j in range(len(reference_tokens) - i + 1)])\n",
        "        cand_ngrams = Counter([' '.join(candidate_tokens[j:j+i])\n",
        "                              for j in range(len(candidate_tokens) - i + 1)])\n",
        "\n",
        "        if sum(cand_ngrams.values()) == 0:\n",
        "            precisions.append(0.0)\n",
        "            continue\n",
        "\n",
        "        matches = sum((ref_ngrams & cand_ngrams).values())\n",
        "        total = sum(cand_ngrams.values())\n",
        "        precision = matches / total\n",
        "        precisions.append(precision)\n",
        "\n",
        "    # Brevity penalty\n",
        "    bp = 1.0\n",
        "    if len(candidate_tokens) < len(reference_tokens):\n",
        "        bp = math.exp(1 - len(reference_tokens) / len(candidate_tokens))\n",
        "\n",
        "    # Geometric mean of precisions\n",
        "    if all(p > 0 for p in precisions):\n",
        "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
        "    else:\n",
        "        bleu = 0.0\n",
        "\n",
        "    return bleu * 100  # Convert to percentage\n",
        "\n",
        "def evaluate_model(model, test_sentences_en, test_sentences_fr,\n",
        "                  en_tokenizer, fr_tokenizer, max_length_en, max_length_fr):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    total_bleu = 0\n",
        "    num_sentences = len(test_sentences_en)\n",
        "\n",
        "    for i in range(num_sentences):\n",
        "        english_sentence = test_sentences_en[i]\n",
        "        reference_french = test_sentences_fr[i]\n",
        "\n",
        "        # Remove start and end tokens from reference\n",
        "        reference_french = reference_french.replace('<start> ', '').replace(' <end>', '')\n",
        "\n",
        "        predicted_french, _, _ = translate(english_sentence, model, en_tokenizer,\n",
        "                                      fr_tokenizer, max_length_en, max_length_fr)\n",
        "        predicted_french = predicted_french.replace('<start> ', '').replace(' <end>', '')\n",
        "\n",
        "        bleu = calculate_bleu_score(reference_french, predicted_french)\n",
        "        total_bleu += bleu\n",
        "\n",
        "        if i < 5:  # Print first 5 examples\n",
        "            print(f\"English: {english_sentence}\")\n",
        "            print(f\"Reference: {reference_french}\")\n",
        "            print(f\"Predicted: {predicted_french}\")\n",
        "            print(f\"BLEU Score: {bleu:.2f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    avg_bleu = total_bleu / num_sentences\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
        "    return avg_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAWO0PCfTAIu"
      },
      "source": [
        "## **Visualization**\n",
        "---\n",
        "### **Attention Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8D0c9diS9Ox"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence, max_length=None):\n",
        "    \"\"\"Plot attention weights\"\"\"\n",
        "    if max_length is None:\n",
        "        max_length = attention.shape[0]\n",
        "\n",
        "    sentence_tokens = sentence.split(' ')\n",
        "    predicted_tokens = predicted_sentence.split(' ')\n",
        "\n",
        "    # Remove end token if present\n",
        "    if '<end>' in predicted_tokens:\n",
        "        end_idx = predicted_tokens.index('<end>')\n",
        "        predicted_tokens = predicted_tokens[:end_idx]\n",
        "        attention = attention[:end_idx]\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "    # Only plot attention for actual tokens\n",
        "    attention_plot = attention[:len(predicted_tokens), :len(sentence_tokens)]\n",
        "\n",
        "    ax.matshow(attention_plot, cmap='Blues')\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence_tokens, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_tokens)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.xlabel('English Input')\n",
        "    plt.ylabel('French Output')\n",
        "    plt.title('Attention Visualization')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_attention_for_sentence(sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr):\n",
        "    \"\"\"Visualize attention for a specific sentence\"\"\"\n",
        "    result, processed_sentence, attention_plot = translate(\n",
        "        sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr)\n",
        "\n",
        "    print(f'Input: {sentence}')\n",
        "    print(f'Prediction: {result}')\n",
        "\n",
        "    plot_attention(attention_plot, processed_sentence, result)\n",
        "\n",
        "    return result, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KJqe28VTR6S"
      },
      "source": [
        "## **Complete Implementation**\n",
        "---\n",
        "\n",
        "### **Main Training Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xYcjD2JYTIph",
        "outputId": "26601c17-8d10-4296-a873-8862929dd2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n",
            "before Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "before Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\n",
            "before Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\n",
            "before Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\n",
            "before Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "before Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "before Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "before Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "before Run!\tPrenez vos jambes à vos cous !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077449 (sacredceltic)\n",
            "before Run!\tFile !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2077454 (sacredceltic)\n",
            "after ['<start> go . <end>', '<start> va ! <end>']\n",
            "after ['<start> go . <end>', '<start> marche . <end>']\n",
            "after ['<start> go . <end>', '<start> en route ! <end>']\n",
            "after ['<start> go . <end>', '<start> bouge ! <end>']\n",
            "after ['<start> hi . <end>', '<start> salut ! <end>']\n",
            "after ['<start> hi . <end>', '<start> salut . <end>']\n",
            "after ['<start> run ! <end>', '<start> cours ! <end>']\n",
            "after ['<start> run ! <end>', '<start> courez ! <end>']\n",
            "after ['<start> run ! <end>', '<start> prenez vos jambes a vos cous ! <end>']\n",
            "after ['<start> run ! <end>', '<start> file ! <end>']\n",
            "English max length: 9\n",
            "French max length: 17\n",
            "English vocab size: 4302\n",
            "French vocab size: 7475\n",
            "Training samples: 24000\n",
            "Validation samples: 6000\n",
            "Starting training...\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'luong_attention_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.4491\n",
            "Epoch 1 Batch 100 Loss 1.7176\n",
            "Epoch 1 Batch 200 Loss 1.6061\n",
            "Epoch 1 Batch 300 Loss 1.4861\n",
            "Epoch 1 Loss 1.7084\n",
            "Time taken for 1 epoch 38.69 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.3894\n",
            "Epoch 2 Batch 100 Loss 1.3725\n",
            "Epoch 2 Batch 200 Loss 1.1505\n",
            "Epoch 2 Batch 300 Loss 1.1260\n",
            "Epoch 2 Loss 1.2528\n",
            "Time taken for 1 epoch 24.76 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1413\n",
            "Epoch 3 Batch 100 Loss 1.0873\n",
            "Epoch 3 Batch 200 Loss 0.9799\n",
            "Epoch 3 Batch 300 Loss 0.8652\n",
            "Epoch 3 Loss 0.9671\n",
            "Time taken for 1 epoch 24.80 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7529\n",
            "Epoch 4 Batch 100 Loss 0.8109\n",
            "Epoch 4 Batch 200 Loss 0.6846\n",
            "Epoch 4 Batch 300 Loss 0.7672\n",
            "Epoch 4 Loss 0.7666\n",
            "Time taken for 1 epoch 24.06 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6197\n",
            "Epoch 5 Batch 100 Loss 0.5811\n",
            "Epoch 5 Batch 200 Loss 0.5980\n",
            "Epoch 5 Batch 300 Loss 0.6544\n",
            "Epoch 5 Loss 0.6117\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Checkpoint saved at epoch 5\n",
            "Epoch 6 Batch 0 Loss 0.4092\n",
            "Epoch 6 Batch 100 Loss 0.4540\n",
            "Epoch 6 Batch 200 Loss 0.4349\n",
            "Epoch 6 Batch 300 Loss 0.4552\n",
            "Epoch 6 Loss 0.4900\n",
            "Time taken for 1 epoch 24.09 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3633\n",
            "Epoch 7 Batch 100 Loss 0.3936\n",
            "Epoch 7 Batch 200 Loss 0.4868\n",
            "Epoch 7 Batch 300 Loss 0.3863\n",
            "Epoch 7 Loss 0.3927\n",
            "Time taken for 1 epoch 24.06 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.3141\n",
            "Epoch 8 Batch 100 Loss 0.2829\n",
            "Epoch 8 Batch 200 Loss 0.3113\n",
            "Epoch 8 Batch 300 Loss 0.3728\n",
            "Epoch 8 Loss 0.3197\n",
            "Time taken for 1 epoch 24.06 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2575\n",
            "Epoch 9 Batch 100 Loss 0.2728\n",
            "Epoch 9 Batch 200 Loss 0.3424\n",
            "Epoch 9 Batch 300 Loss 0.3013\n",
            "Epoch 9 Loss 0.2637\n",
            "Time taken for 1 epoch 24.10 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2469\n",
            "Epoch 10 Batch 100 Loss 0.1918\n",
            "Epoch 10 Batch 200 Loss 0.2266\n",
            "Epoch 10 Batch 300 Loss 0.2120\n",
            "Epoch 10 Loss 0.2226\n",
            "Time taken for 1 epoch 24.08 sec\n",
            "\n",
            "Checkpoint saved at epoch 10\n",
            "Epoch 11 Batch 0 Loss 0.1614\n",
            "Epoch 11 Batch 100 Loss 0.1787\n",
            "Epoch 11 Batch 200 Loss 0.1695\n",
            "Epoch 11 Batch 300 Loss 0.1999\n",
            "Epoch 11 Loss 0.1919\n",
            "Time taken for 1 epoch 24.08 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1559\n",
            "Epoch 12 Batch 100 Loss 0.1351\n",
            "Epoch 12 Batch 200 Loss 0.1807\n",
            "Epoch 12 Batch 300 Loss 0.1603\n",
            "Epoch 12 Loss 0.1711\n",
            "Time taken for 1 epoch 24.08 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1351\n",
            "Epoch 13 Batch 100 Loss 0.1757\n",
            "Epoch 13 Batch 200 Loss 0.1279\n",
            "Epoch 13 Batch 300 Loss 0.1829\n",
            "Epoch 13 Loss 0.1547\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0929\n",
            "Epoch 14 Batch 100 Loss 0.1607\n",
            "Epoch 14 Batch 200 Loss 0.1346\n",
            "Epoch 14 Batch 300 Loss 0.1579\n",
            "Epoch 14 Loss 0.1428\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0894\n",
            "Epoch 15 Batch 100 Loss 0.1013\n",
            "Epoch 15 Batch 200 Loss 0.1271\n",
            "Epoch 15 Batch 300 Loss 0.1605\n",
            "Epoch 15 Loss 0.1335\n",
            "Time taken for 1 epoch 24.10 sec\n",
            "\n",
            "Checkpoint saved at epoch 15\n",
            "Epoch 16 Batch 0 Loss 0.0731\n",
            "Epoch 16 Batch 100 Loss 0.1397\n",
            "Epoch 16 Batch 200 Loss 0.1146\n",
            "Epoch 16 Batch 300 Loss 0.1544\n",
            "Epoch 16 Loss 0.1281\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0851\n",
            "Epoch 17 Batch 100 Loss 0.1227\n",
            "Epoch 17 Batch 200 Loss 0.1157\n",
            "Epoch 17 Batch 300 Loss 0.1326\n",
            "Epoch 17 Loss 0.1250\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0930\n",
            "Epoch 18 Batch 100 Loss 0.1032\n",
            "Epoch 18 Batch 200 Loss 0.1525\n",
            "Epoch 18 Batch 300 Loss 0.1389\n",
            "Epoch 18 Loss 0.1208\n",
            "Time taken for 1 epoch 24.06 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0784\n",
            "Epoch 19 Batch 100 Loss 0.1368\n",
            "Epoch 19 Batch 200 Loss 0.1038\n",
            "Epoch 19 Batch 300 Loss 0.1082\n",
            "Epoch 19 Loss 0.1187\n",
            "Time taken for 1 epoch 24.07 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1080\n",
            "Epoch 20 Batch 100 Loss 0.1159\n",
            "Epoch 20 Batch 200 Loss 0.0907\n",
            "Epoch 20 Batch 300 Loss 0.1288\n",
            "Epoch 20 Loss 0.1168\n",
            "Time taken for 1 epoch 24.10 sec\n",
            "\n",
            "Checkpoint saved at epoch 20\n",
            "Training completed and model saved!\n",
            "\n",
            "==================================================\n",
            "TESTING TRANSLATIONS\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Bidirectional' object has no attribute 'layer'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1119458108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1119458108.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mvisualize_attention_for_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3839551961.py\u001b[0m in \u001b[0;36mevaluate_translation\u001b[0;34m(sentence, model, en_tokenizer, fr_tokenizer, max_length_fr)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m\"\"\"Evaluate a single translation\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     result, sentence, attention_plot = translate(sentence, model, en_tokenizer,\n\u001b[0m\u001b[1;32m     43\u001b[0m                                                fr_tokenizer, max_length_fr)\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3839551961.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, model, en_tokenizer, fr_tokenizer, max_length_fr)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0men_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Bidirectional' object has no attribute 'layer'"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Hyperparameters\n",
        "    BATCH_SIZE = 64\n",
        "    embedding_dim = 256\n",
        "    units = 512\n",
        "    EPOCHS = 20\n",
        "\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "\n",
        "    # Load dataset (replace with your dataset path)\n",
        "    en, fr = create_dataset('/content/fra.txt', num_examples=30000)\n",
        "\n",
        "    # Create tokenizers and datasets\n",
        "    input_tensor, target_tensor, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr = create_tokenizers_and_datasets(en, fr)\n",
        "\n",
        "    # Split dataset\n",
        "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "        input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"Training samples: {len(input_tensor_train)}\")\n",
        "    print(f\"Validation samples: {len(input_tensor_val)}\")\n",
        "\n",
        "    # Create tf.data.Dataset\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = EncoderDecoderModel(\n",
        "        enc_vocab_size=len(en_tokenizer.word2idx),\n",
        "        dec_vocab_size=len(fr_tokenizer.word2idx),\n",
        "        embedding_dim=embedding_dim,\n",
        "        enc_units=units,\n",
        "        dec_units=units,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Train model\n",
        "    train_model(model, dataset, EPOCHS, steps_per_epoch)\n",
        "\n",
        "    # Save final model\n",
        "    model.save_weights('checkpoints/final_model.weights.h5')\n",
        "    print(\"Training completed and model saved!\")\n",
        "\n",
        "    # Test translation\n",
        "    test_sentences = [\n",
        "        \"I love you.\",\n",
        "        \"How are you?\",\n",
        "        \"Good morning.\",\n",
        "        \"This is a beautiful day.\",\n",
        "        \"Where is the bathroom?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TESTING TRANSLATIONS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        result, attention_plot = evaluate_translation(sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr)\n",
        "        visualize_attention_for_sentence(sentence, model, en_tokenizer, fr_tokenizer, max_length_en, max_length_fr)\n",
        "        print()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATING ON VALIDATION SET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Convert validation tensors back to sentences for BLEU evaluation\n",
        "    val_en_sentences = [en_tokenizer.decode(seq) for seq in input_tensor_val[:100]]\n",
        "    val_fr_sentences = [fr_tokenizer.decode(seq) for seq in target_tensor_val[:100]]\n",
        "\n",
        "    avg_bleu = evaluate_model(model, val_en_sentences, val_fr_sentences,\n",
        "                            en_tokenizer, fr_tokenizer, max_length_en, max_length_fr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb9CRuFcTic0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
